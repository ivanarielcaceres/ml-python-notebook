{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Build a sentiment analysis / polarity model\n",
    "Sentiment analysis can be casted as a binary text classification problem,\n",
    "that is fitting a linear classifier on features extracted from the text\n",
    "of the user messages so as to guess wether the opinion of the author is\n",
    "positive or negative.\n",
    "In this examples we will use a movie review dataset.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 2000\n"
     ]
    }
   ],
   "source": [
    "movie_reviews_data_folder = r'C:\\Users\\Administrator\\nltk_data\\corpora\\movie_reviews'\n",
    "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
    "print(\"n_samples: %d\" % len(dataset.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the dataset in training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "       dataset.data, dataset.target, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
    "that are too rare or too frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
    "        ('clf', LinearSVC(C=1000)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Build a grid search to find out whether unigrams or bigrams are\n",
    "more useful.\n",
    "Fit the pipeline on the training set using grid search for the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    }\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: print the mean and std for each candidate along with the parameter\n",
    "settings for all the candidates explored by grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.83; std - 0.02\n",
      "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.85; std - 0.02\n"
     ]
    }
   ],
   "source": [
    "n_candidates = len(grid_search.cv_results_['params'])\n",
    "for i in range(n_candidates):\n",
    "    print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
    "             % (grid_search.cv_results_['params'][i],\n",
    "                grid_search.cv_results_['mean_test_score'][i],\n",
    "                grid_search.cv_results_['std_test_score'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK: Predict the outcome on the testing set and store it in a variable\n",
    "named y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicted = grid_search.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.86      0.84      0.85       250\n",
      "        pos       0.85      0.86      0.86       250\n",
      "\n",
      "avg / total       0.85      0.85      0.85       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_predicted,\n",
    "                                    target_names=dataset.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and plot the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[211  39]\n",
      " [ 34 216]]\n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n96SzoTzzGhMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xfafc77fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying classifier on fake movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_new = ['This movie was excellent', 'Absolute joy ride', \n",
    "            'Steven Seagal was terrible', 'Steven Seagal shined through.', \n",
    "              'This was certainly a movie', 'Two thumbs up', 'I fell asleep halfway through', \n",
    "              \"We can't wait for the sequel!!\", '!', '?', 'I cannot recommend this highly enough', \n",
    "              'instant classic.', 'Steven Seagal was amazing. His performance was Oscar-worthy.',\n",
    "              'This project is the best of the year',\n",
    "              'This project sucks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This movie was excellent' => pos\n",
      "'Absolute joy ride' => pos\n",
      "'Steven Seagal was terrible' => neg\n",
      "'Steven Seagal shined through.' => neg\n",
      "'This was certainly a movie' => neg\n",
      "'Two thumbs up' => neg\n",
      "'I fell asleep halfway through' => neg\n",
      "\"We can't wait for the sequel!!\" => neg\n",
      "'!' => neg\n",
      "'?' => neg\n",
      "'I cannot recommend this highly enough' => neg\n",
      "'instant classic.' => pos\n",
      "'Steven Seagal was amazing. His performance was Oscar-worthy.' => neg\n",
      "'This project is the best of the year' => pos\n",
      "'This project sucks' => neg\n"
     ]
    }
   ],
   "source": [
    "predict = grid_search.predict(reviews_new)\n",
    "for review, category in zip(reviews_new, predict):\n",
    "    print('%r => %s' % (review, dataset.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
